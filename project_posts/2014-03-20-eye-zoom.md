Still planning last push (for now) on “inner voice” (plan-A).  Added alpha synchronous sounds (phonemes) and subliminal words and pictures to the code yesterday.  Plan is to throw the kitchen sink at it and see if I’m getting anything over the weekend.

More thoughts about the algorithm for “eye zoom” (plan-B):

Yesterday I eliminated the idea of continuous pan/zoom, but this morning I’m reopening that possibility.   I’m a little concerned that the idea of discrete pan/zoom events will not feel enough like the user is controlling it directly.  The sense of agency is very important.   People are not used to controlling things with their eyes, so the first impression may be that the computer is controlling things.  I could make the discrete pan/zoom seem more user-controlled by reducing the number of fixations between pan/zoom events.  The problem is that will also reduce the likelihood of producing the correct pan/zoom.  Maybe continuous would be ok if it were very slow.  Even with a slow velocity, the velocity vector can change instantaneously to give feedback to the user.

The simplest continuous algorithm would be to always pan so as to center the current eye position.  For the zoom there are more options since you have to consider more than one eye position.  The simplest would be to zoom based on the previous and current eye positions.  If the distance between them is smaller then zoom in, if it is larger than a certain amount zoom out.  Because the pan/zoom is very slow there would be a built-in smoothing of the eye data, but still the user would see instantaneous control.  Having discontinuous changes in the velocity of the pan/zoom may not be comfortable visually, so there may need to be a little temporal smoothing there.

The two ideas could be combined, and maybe that would be the best (but not the simplest!).   The slow pan/zoom could always be active, and when enough evidence accumulates that the user is mostly interested in a smaller region, then a fast pan/zoom event would happen.  This could be formulated as a conditional probability: the probability that the next n fixations will be in a specific region given that the previous m fixations where in that region.    With a database of real data for a set of gigapixel images, one could make tables of these probabilities for different region sizes and values of n and m.  It would be useful to know to what degree these tables are different between people and between images.  With these tables, once you set the probability cutoff, you could know whether it was worth zooming.  With a bigger database one could characterize differences between images and users (maybe using PCA) allowing on-line selection of which probability table is appropriate.  To tailor the algorithm to each user, one could have a single (carefully selected) test image that is used to select user specific tables.  The user could be characterized along a small number of principle components.  These could be back projected to create the optimal conditional probability table for that user.

What I am pretty sure what I don’t want is to have continuous zoom with variable velocity.  It is very easy to feel out of control when things are speeding up on you.  By combining slow fixed pan/zoom with larger discrete camera moves, I think I can get the best of both worlds.  It won’t become frustrating because of the slowness, but there will still be fine control.

I just saw that someone has been making gigapixel images of faces.  You could really fly into someone’s pores.  Could be fun and maybe somewhat disgusting.  Also I was listening to a photography podcast that was interviewing this guy who takes pictures of people using the world’s largest camera.   It is really a moving van sized thing.  I’m sure the pictures must be in the terapixels.  Also there are gigapixel images of the moon that would be fun to explore a little.

 There are whole websites of people making gigapixel images using robotic camera rigs (just search for gigapixel).  I have one (gigapan) that I have played with a little bit, so I may try making some of my own images for this project.  I saw someone using one while I was photographing at Arches National Monument.  They are not very expensive.  

Of course the next step would be 3D gigapixel...



