I’ve started to put together a demo using opengaze and GazerCocoa.  These use a simple webcam, so they are not high quality eye tracking.  I figure if I can make anything workable with this, then it could only be better with real eye tracking hardware.

There is at least two big problems with having gaze controlled pan/zoom: noise in the eye movement signal (from the eye tracker and from the eye/brain), and psychological difficulty of seeing things moving around “by themselves” (sense of agency).  The obvious answer for both of these would be to time averaging (temporal smoothing) of some sort.  

Very preliminary eye gaze control pseudo-code:

```
update_transform(Point eye_position)
{
	delta_eye_vector = eye_postition – previous_eye_position;
	eye_vector_length = length(delta_eye_vector);
	eye_unit_vector = delta_eye_vector / eye_vector_length;

	// allow fine control and long eye movements without much movement
	translation_length = 1.0 - exp(-fine_control * eye_vector_length);

	translation = (1.0 – translation_smoothing) * previous_translation
		+ translation_smoothing * translation_gain * translation_length * eye_unit_vector;
	// increasing eye movement lengths gradually zoom out, decreasing zooms in
	zoom = (1.0 – zoom_smoothing) * previous_zoom
		- zoom_smoothing * zoom_gain * (eye_vector_length – previous_eye_vector_length);
	previous_zoom = zoom;
	previous_eye_position = eye_position;
	previous_translation = translation;
}
```

On top of this, it would be important to limit the rate at which the direction of translation or zoom can change (the acceleration should be smooth, especially around zero).



Ganzfeld update

The EL sheets work very nicely to give a uniform illumination.  They are flexible so they easily warp around the face.  They are about the right brightness.  Much better than using ping-pong balls.  After trying them for about 15 minutes, I did see the brightness start fluctuating, but no imagery from the depths of my mind.  

Using the one eye covered technique; I did start seeing some interesting heat diffusion like patterns.  That seems to imply something about the differential equations that are operating in early visual areas.  This could have some implications for modeling that part of the brain.

I tried a number of variations on the random noise visual and sound that is pulsed at low alpha frequency, then ramped to higher frequencies.  I feel like it could work, and maybe I’m getting closer.  I am starting to see some strange things, but nothing definite.


