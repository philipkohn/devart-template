Plan-A

I did some more experiments.  I added subliminal pictures of objects that are added to the checkerboard only during one phase of the flickering.  The idea is to attempt to entrain higher visual and spatial areas to the ramping alpha frequency.  Each subliminal video frame contains from 0 to 6 objects.  I know from fMRI experiments that should modulate a lot of posterior cortical areas including parietal areas that are involved in representing quantities.  I think I might be starting to see something strange happening, but it is hard to know if I'm fooling myself at this point.  I did also notice a strange effect when I switched which eye I was covering.  It seems like it takes a while to re-adjust, and during that time I am still seeing a big blackout area in my central vision.  It might be worth exploring this.   I got the red EL panels yesterday and I plan to try those this weekend.  During the week my plan is to add subliminal words and numbers to the demo program.

Plan-B

I am pretty excited about the possibilities of eye movement guided viewing of gigapixel images.  I'll have to check the patents, because this idea is not strange enough that it hasn't been thought of by many people.  Still I may be able to add something with a specific algorithm for transforming eye position history into a series of image transforms.  This is where the art, user interface design and engineering come together.

Thoughts about the algorithm:

I don't think it will be possible given what I've seen with eye movement data (and I have done a fair amount of this for my work) to predict future eye positions reliability enough.  Here the user interface principle of minimizing surprise comes into play.  The cost of making a mistake is too high.

The next question is time scale.  I don't think we can zoom/pan at a time scale near the rate of saccades.  It would be too much for most people to handle.  The actual image on the retina is moving around very rapidly, but we see a relatively stable perception of the world.  The theory is that there is an “efferent copy” of the motor signal that is moving the eyes that somehow adjusts the visual system to put together this bigger picture.  If you move the eye manually, or get drunk enough that you eyes are not being controlled very well, then this breaks down and you see the whole world moving.  Generally this is not a pleasant thing.  In the far future it might be possible to tap into this system and give it the right information to create a spatially stable virtual world with gigapixel resolution.  For the current system I think the time scale is going to be relatively slow.  That will be an important parameter of the system that may need to be tuned to each individual.  I would guess that it should be something on the order of 2-8 seconds.  Think about the way movies are edited.  (I think I read that it is a 1/f distribution of shot lengths.)  Movie editors have learned how to make cuts, zooms and pans so natural that you don't even notice them (unless they want you to notice them).  There is no doubt a lot of information useful to this project in that domain.

Is there a way to evaluate the system automatically?  This would allow on-line tuning of the parameters, and more importantly to know when a change is for the better.  For each pan/zoom there could be a period of time that the transform is fixed.  The closer the eye positions are to the middle of the screen at that point, the better the pan part of the transform.  For zoom it is a little tricker.  If the average radius (or standard deviation of distance from the centroid) of eye positions changes, then what does that mean?   If you zoom in to a detail of interest, you'd expect the spread of eye positions to increase in size.  When  you zoom out correctly, there should be more fixations near the edges.

What are the parameters that could be extracted from the data as input for the algorithm?
If we ignore the order of fixations and just look at the set of eye positions, then we can do spatial clustering and get some measures such as the number of clusters, their average sizes and distances between them.  I just realized that segmenting the image being viewed would allow the clusters to be made on the bases of objects in the image.  It would be nice to pan/zoom in a way that respects the objects in the scene.  Given that segmentation is not easy to do well, I'll leave that for later.  Clustering is not my favorite algorithm either because there are so many ways to do it that will give you quite different results.  The fundamental problem is that clustering requires categorical decisions, which may not be a good model of the system, and can be brittle to small changes in the data or the parameters of the model.  Also it has hard to define what the “right” answer should be.  Maybe forget the clustering, and just look at more robust measures of the spread and centroid. 

Rather than a fixed window of time, it might be better to have a weighted window that emphasizes the recent over the past (now putting order of fixations back in a parameterized way).

What is the simplest algorithm that has any chance of working?

Say that we take the bounding rectangle of all the eye positions in the last 10 seconds.  If the rectangle is smaller than a certain size, then pan/zoom that rectangle to full screen.  If it is larger then pan/zoom out around the center of the rectangle and amount relative to how big the bounding eye position  rectangle is.


Thoughts about presentation:

The old peep-show viewers are really rare and super expensive.  I couldn't find any reproductions, but I'd guess they exist.  I am wondering if this may be too strange, or too artsy.

The other approach would be to go for something hyper-modern.  To make this practical, I think I will have to use an eye tracker that is made to be worn on the head.  There are really nice trackers that operate at a distance, but they are super expensive at this point.  (With 4k video and processing hardware becoming cheap, I'm guessing that soon there will be inexpensive camera based eye trackers that could even do several people at once.)   I really don't want to have glasses, so it is going to be something you look into.  I don't want a regular kiosk for the same reasons as not having obvious computer monitors: too many connotations that take away from the art.  It could be something minimalistic like a very small thin object, not much bigger than needed to cover the eyes.  It might be held up by an automatically telescoping rod that adjusts for the viewer's height when they step near it.

I'm trying to think of other contexts when we look through something.  The classic holes in a wooden fence. The connotation would be voyeuristic, which I sort of like because zooming way into a scene has that quality.  It would be a lot more surprising to see a gigapixel view in this context than in some strange hyper-modern thing.
